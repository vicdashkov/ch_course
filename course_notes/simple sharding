TODO: time it

Why do we need cluster?
- scalability (sharding or segmentation) - comes with CH
- reliability (replication) - Zookeeper

Experiment - 1 Node running; adding one node

Let's update our docker so it looks like this (added ch2 and renaming)
```
version: '3'

services:
  ch1:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch1/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ports:
      - 9000:9000
      - 8123:8123
    ulimits:
      nofile: 262144

  ch2:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch2/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ports:
      - 9001:9000
      - 8124:8123
    ulimits:
      nofile: 262144

  client:
    image: yandex/clickhouse-client:18.14.15
```

Executing should return empty
select * from system.clusters

Let's add this segment and run select * from system.clusters
<remote_servers>
    <segmented>
        <shard>
            <replica>
                <host>ch1</host>
                <port>9000</port>
            </replica>
        </shard>
        <shard>
            <replica>
                <host>ch2</host>
                <port>9000</port>
            </replica>
        </shard>
    </segmented>
</remote_servers>
Now we see. Good start
┌─cluster───┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┬─user────┬─default_database─┐
│ segmented │         1 │            1 │           1 │ ch1       │ 172.23.0.2   │ 9000 │        1 │ default │                  │
│ segmented │         2 │            1 │           1 │ ch2       │ 172.23.0.4   │ 9000 │        0 │ default │                  │
└───────────┴───────────┴──────────────┴─────────────┴───────────┴──────────────┴──────┴──────────┴─────────┴──────────────────┘

Writing data 2 ways:
1) Write to local (docs say optimal)
2) Write to distributed

Let's take a look at the table structure I want to use

CREATE TABLE pokemon.event_1
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

CREATE TABLE pokemon.event_1_distributed
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = Distributed(segmented, pokemon, event_1, rand())

insert into event_1_distributed values(122, '2018-01-01 00:00:00', 33, 2222)

TODO: FILE A BUG FOR THIS?
NOTE: if local tables are not created on all nodes, CH will through an error when select. Insert will work

Now let's simulate hot changing from single to multinode cluster. drop all the tables and start clean
select * from system.clusters
should be empty

First create event_1 again on ch1
Then start writer
Adjust configs
Create event_1_distributed on ch1
Then event_1 and event_1_distributed on ch2
We see that distributed table is filling up, but local tables on ch2 is not
This may be your desired behaviour, and CH docs reccomended it
If you want to delegate insertion to local table to CH, setup all the tables up front

Lets do that and hot-add 3rd node to our cluster
Our docker compose looks like this:

...
ch3:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch3/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ulimits:
      nofile: 262144
...

truncating event_1 on ch1. All should be empty now.
Starting inserter. Looking at clusters. Adding one more node to cluster in configs
Creating event_1 and event_1_distributed on the ch3
Checking and we see that inserter started to write to node already
Everything works as expected and super easy to setup

Lastly, Let's take a look at altering distributed table
Cool query won't work untill zookeeper
ALTER TABLE event_1_distributed ON CLUSTER segmented ADD COLUMN location_id DEFAULT 0

So we'll have to execute it by ourselves
first on every node:
ALTER TABLE event_1 ADD COLUMN location_id UInt16
then
ALTER TABLE event_1_distributed ADD COLUMN location_id UInt16
insert into event_1_distributed values(122, '2018-01-01 00:00:00', 33, 2222,4)

NOTE: if alter only one one node, insert and select will work, but only in format defined on each node.
Make sure to modify distributed table on every node.

Now just for fun lets measure select from distributed tables and from single node table.
On ch1 let's create
CREATE TABLE pokemon.event_1_single_node
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16,
  location_id UInt16
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);
Lets insert 1850 events to distributed talbes and to single node talble

RESULTS OF INSERTS:
2018-12-25 09:30:46 - table: event_1_distributed - inserted: 18250 - took: 366.77 - bulk size: 5 - events per day: 50 - workers: 10
2018-12-25 09:40:45 - table: event_1_single_node - inserted: 18250 - took: 490.9 - bulk size: 5 - events per day: 50 - workers: 10

RESULTS OF SELECTS: TODO: maybe redo with docker limits
 2018-12-25 09:43:28
SELECT count() from pokemon.event_1_single_node group by toYYYYMM(time)
 attempt 0 took: 0.74587s
 attempt 1 took: 0.45294s
 attempt 2 took: 0.51253s

 2018-12-25 09:44:06
SELECT count() from pokemon.event_1_distributed group by toYYYYMM(time)
 attempt 0 took: 1.25376s
 attempt 1 took: 1.01273s
 attempt 2 took: 0.98734s

 2018-12-25 09:45:41
SELECT count() from pokemon.event_1_distributed group by toYYYYMM(time)
 attempt 0 took: 1.22017s
 attempt 1 took: 0.80592s
 attempt 2 took: 0.95683s

 2018-12-25 09:45:58
SELECT count() from pokemon.event_1_single_node group by toYYYYMM(time)
 attempt 0 took: 0.74441s
 attempt 1 took: 0.53134s
 attempt 2 took: 0.44611s

