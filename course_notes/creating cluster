TODO: time it

Why do we need cluster?
- scalability (sharding or segmentation) - comes with CH
- reliability (replication) - Zookeeper

Experiment - 1 Node running; adding one node

Let's update our docker so it looks like this (added ch2 and renaming)
```
version: '3'

services:
  ch1:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch1/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ports:
      - 9000:9000
      - 8123:8123
    ulimits:
      nofile: 262144

  ch2:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch2/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ports:
      - 9001:9000
      - 8124:8123
    ulimits:
      nofile: 262144

  client:
    image: yandex/clickhouse-client:18.14.15
```

Executing should return empty
select * from system.clusters

Let's add this segment and run select * from system.clusters
<remote_servers>
    <segmented>
        <shard>
            <replica>
                <host>ch1</host>
                <port>9000</port>
            </replica>
        </shard>
        <shard>
            <replica>
                <host>ch2</host>
                <port>9000</port>
            </replica>
        </shard>
    </segmented>
</remote_servers>
Now we see. Good start
┌─cluster───┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┬─user────┬─default_database─┐
│ segmented │         1 │            1 │           1 │ ch1       │ 172.23.0.2   │ 9000 │        1 │ default │                  │
│ segmented │         2 │            1 │           1 │ ch2       │ 172.23.0.4   │ 9000 │        0 │ default │                  │
└───────────┴───────────┴──────────────┴─────────────┴───────────┴──────────────┴──────┴──────────┴─────────┴──────────────────┘

Writing data 2 ways:
1) Write to local (docs say optimal)
2) Write to distributed

Let's take a look at the table structure I want to use

CREATE TABLE pokemon.event_1
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

CREATE TABLE pokemon.event_1_distributed
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = Distributed(segmented, pokemon, event_1, rand())

insert into event_1_distributed values(122, '2018-01-01 00:00:00', 33, 2222)

Now let's simulate hot changing from single to multinode cluster. drop all the tables and start clean
select * from system.clusters
should be empty

First create event_1 again on ch1
Then start writer
Adjust configs
Create event_1_distributed on ch1
Then event_1 and event_1_distributed on ch2
We see that distributed table is filling up, but local tables on ch2 is not
This may be your desired behaviour, and CH docs reccomended it
If you want to delegate insertion to local table to CH, setup all the tables up front

Lets do that and hot-add 3rd node to our cluster
Our docker compose looks like this:

...
ch3:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    volumes:
      - './data_ch3/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ulimits:
      nofile: 262144
...

truncating event_1 on ch1. All should be empty now.
Starting inserter. Looking at clusters. Adding one more node to cluster in configs
Creating event_1 and event_1_distributed on the ch3
Checking and we see that inserter started to write to node already
Everything works as expected and super easy to setup

Lastly, Let's take a look at altering distributed table
Cool query won't work untill zookeeper
ALTER TABLE event_1_distributed ON CLUSTER segmented ADD COLUMN location_id DEFAULT 0

So we'll have to execute it by ourselves
first on every node:
ALTER TABLE event_1 ADD COLUMN location_id UInt16
then
ALTER TABLE event_1_distributed ADD COLUMN location_id UInt16
TODO: try to insert if alter only on one node
insert into event_1_distributed values(122, '2018-01-01 00:00:00', 33, 2222,4)