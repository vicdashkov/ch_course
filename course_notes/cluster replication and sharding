// todo: pokemon is not the best name for the cluster; at least with pokemon db; confusing
// todo: in system.replicas only 1 replica shows up why?

Replication works at the level of an individual table, not the entire server.
CREATE, DROP, ATTACH, DETACH and RENAME queries are executed on a single server and are not replicated
Replicated tables can have different names on different replicas.
Insert data in batches of no more than one INSERT per second

Ok lets add remove_server.xml to config.d dir with this
<yandex>
    <remote_servers replace="true">
        <pokemon>
            <!-- shard 01 -->
            <shard>
                <replica>
                    <host>ch1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>ch2</host>
                    <port>9000</port>
                </replica>
            </shard>

            <!-- shard 02 -->
            <shard>
                <replica>
                    <host>ch3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>ch4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </pokemon>
    </remote_servers>
</yandex>

and to zookeeper.xml add this
<yandex>
    <zookeeper replace="true">
        <node index="1">
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>
</yandex>

Our docker-compose looks like
version: '3'

services:

  zookeeper:
    image: zookeeper
    volumes:
      - './data_zoo/data:/data'
      - './data_zoo/datalog:/datalog'

  ch1:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    depends_on:
      - zookeeper
    volumes:
      - './data_ch1/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ports:
      - 9000:9000
      - 8123:8123
    ulimits:
      nofile: 262144

  ch2:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    depends_on:
      - zookeeper
    volumes:
      - './data_ch2/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ulimits:
      nofile: 262144

  ch3:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    depends_on:
      - zookeeper
    volumes:
      - './data_ch3/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ulimits:
      nofile: 262144

  ch4:
    image: yandex/clickhouse-server:18.14.15
    restart: on-failure
    depends_on:
      - zookeeper
    volumes:
      - './data_ch4/ch:/var/lib/clickhouse/'
      - './ch_configs:/etc/clickhouse-server/'
    ulimits:
      nofile: 262144

  client:
    image: yandex/clickhouse-client:18.14.15

-- You can optionally specify macros which will substitute {} during table creation.
-- We will omit the since we have single configs for all the nodes (don't do it in prod)

run compose up; connect to cluster with
docker-compose run --rm client --host ch1 -u vic --password 12345 --database=pokemon
execute
SELECT * FROM system.clusters
we should see
┌─cluster─┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┬─user────┬─default_database─┐
│ pokemon │         1 │            1 │           1 │ ch1       │ 172.23.0.5   │ 9000 │        1 │ default │                  │
│ pokemon │         1 │            1 │           2 │ ch2       │ 172.23.0.4   │ 9000 │        1 │ default │                  │
│ pokemon │         2 │            1 │           1 │ ch3       │ 172.23.0.6   │ 9000 │        0 │ default │                  │
│ pokemon │         2 │            1 │           2 │ ch4       │ 172.23.0.3   │ 9000 │        0 │ default │                  │
└─────────┴───────────┴──────────────┴─────────────┴───────────┴──────────────┴──────┴──────────┴─────────┴──────────────────┘

-- let's create replicated table on ch1
CREATE TABLE pokemon.event_2
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/1/event', 'ch1')
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

-- let's try to insert
insert into event_2 values(122, '2018-01-01 00:00:00', 33, 2222)

-- now let's create replicated table on ch2
CREATE TABLE pokemon.event_2
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/1/event', 'ch2')
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

-- run
select * from event_2
-- and our data is here. Awesome!

-- let's create this table on to other nodes with
CREATE TABLE pokemon.event_2
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/2/event', 'ch3')
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

-- insert test data
insert into event_2 values(1777, '2018-01-01 00:00:00', 7, 777)

-- and create last table
CREATE TABLE pokemon.event_2
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/2/event', 'ch4')
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

-- select from ch4 should show our data

-- and let's wrap up with distributed table
CREATE TABLE pokemon.event_2_distributed
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = Distributed(pokemon, pokemon, event_2, rand())

-- insert into distributed
insert into event_2_distributed values(22222222, '2018-01-01 00:00:00', 2, 222)
-- and one more time
insert into event_2_distributed values(4444444, '2018-01-01 00:00:00', 4444, 444444)
-- and we see our data replication and sharding works as expected. Great success (to those of you fans of Borat; I'm gettin' old)

-- as a wrap up let's try to execute our distributed ddl query on event_2_distributed
-- NOTE: don't forget pokemon. unless default database is specified
ALTER TABLE pokemon.event_2_distributed ON CLUSTER pokemon ADD COLUMN location_id DEFAULT 0

-- quick run on shows discrepancy
desc table event_2 -- no change
desc table event_2_distributed -- same on all nodes

-- this will return an error; so we still need to execute alter on all the local tables (one time per replica)
-- note again that CREATE, DROP, ATTACH, DETACH and RENAME will be executed on per node
select * from event_2_distributed
ALTER TABLE pokemon.event_2 ADD COLUMN location_id DEFAULT 0

-- checking with; and all is good
SELECT * FROM event_2_distributed