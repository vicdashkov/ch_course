Preparation:
* rm data dir
* rm preprocessed configs
* slides
* vs with course notes

Welcome back everyone.

Sometimes we all have a lot of data to process. khm khm
Which is all well and good. But not always do we have ram to process it.

Ch has an answer here
If we enable sampling on our table, we can save on RAM quite a bit,
since CH will perform your query on a small subset of our data.

ok. lets start by creating regular table

-- fastworward til execute all commands

ok so what we saw is of course regular select works,
But sampling not so much.
To fix it, we need to of course enable sampling

To fix it wee need to create proper sampling table

-- execute create table and inserts and selects

Now, when we select from this table ch will return an evenly
pseudorandom data sample.

ok sort of makes sense. let's benchmark a bit

To do that we'll need to gather one more metric
Query Memory usage

-- execute query

to enable this query we'll need to add a flag in user configs

-- show users.xml

Having done all that I procided to bencmarking
I was operating with with regular 365000000 events and
results were
Dramatic pause ...

... I couldn't make sampling to work properly.
There's the file called called sampling_investigation.

-- show the file

Here I outline my process.
Hopefully it makes for an entertaining read.

I'll just summarize by saying that
I couldn't quite see any performance benefits from using sampling

CH says that there will be none,
but event
Memory consuption didn't go down
Good news is that if you dont use "sample" in your select query
table with sampling enabled will work just as fine and won't take
much more space as well.

-- switch to summary slide

To summorize I would strongly suggest to benchmark your schema.
maybe you'll find a good
combination of primary key and order by fields to make it work.
Plus maybe it will work for your more complex select statements

Unfortunately, I coudn't make it to work with our toy example.
Have different experience? Please let me know!


See you next time