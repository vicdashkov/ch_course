Preparation:
* open slides on the left
* open code editor on the righ
* start ch in code editor

DO: demonstrate SHOW PROCESSLIST

Hello everyone,
In this lesson we'll talk about merge-tree family of CH engines

CH team reccomends using engines of this family pretty much for most tasks
They say those engines are the most advanced engines,
and data replication is possible only with engines from this family

There are other engines as well, and we'll talk about them, but chances are
you'll start with some kind of merge tree table and stick to it for a long time

Let's connect to our newly configured ch instance
and try to create a table and a database

CREATE DATABASE merge_tree

CREATE TABLE merge_tree.event
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = MergeTree();
I was tempted to create a table like so at first. cool kinds don't read the docs, ha
But yeah this will fail

and the reason why it fails is that we need to let CH know about
some of the extra meta data about our table
These configs will be the main focus of this lecture

If we look at this slide we clearly see the difference
one table has date field and the other one doesn't.
The deal is we have to specify order by columns and partition by columns,
and I've seen people use date column for partition,
so essentially we'd have to store 2 date columns: time and date.
Now documentation is pretty clear on it, but in older days this was what people did
I think we can benchmark different setup and then try to make sense of it
I'be prepared different combinations for paprtitioning and order by in this
gist:
https://gist.github.com/vicdashkov/40e8892a878beb866d787316a3a52a29

It's about time I intorduce you to 2 small scripts to kind of dirty
benchmark our tables:
TODO: LINK TO REPO

ok. I'll run the test and we'll come back to results once it's done

-- from https://gist.github.com/sanchezzzhak/511fd140e8809857f8f1d84ddb937015
SELECT table, formatReadableSize(size) as size, rows, days, formatReadableSize(avgDaySize) as avgDaySize FROM (
    SELECT
        table,
        sum(bytes) AS size,
        sum(rows) AS rows
    FROM system.parts
    WHERE active
    GROUP BY table
    ORDER BY rows DESC
)

┌─table───────────────────┬─size───────┬─rows─┬
│ event_0_time_batch      │ 144.05 KiB │ 8900 │
│ event_0_date_batch      │ 416.81 KiB │ 8900 │
│ event_0_time_func_batch │ 142.65 KiB │ 8900 │
│ event_0_date_single     │ 308.38 KiB │ 1780 │
└─────────────────────────┴────────────┴──────┴

2018-12-16 15:13:16 - table: event_0_date_batch - inserted: 8900 - took: 168.58 - bulk size: 5 - events per day: 25 - workers: 10
2018-12-16 15:18:52 - table: event_0_date_single - inserted: 1780 - took: 166.67 - bulk size: 1 - events per day: 5 - workers: 10
2018-12-16 15:25:06 - table: event_0_time_batch - inserted: 8900 - took: 155.42 - bulk size: 5 - events per day: 25 - workers: 10
2018-12-16 15:29:01 - table: event_0_time_func_batch - inserted: 8900 - took: 159.49 - bulk size: 5 - events per day: 25 - workers: 10

SELECT count() from event_0_time_batch group by toYYYYMM(time) 2.677s 2.240s 3.273s
SELECT count() from event_0_date_batch group by toYYYYMM(date) 2.219s 1.797s 2.073s
SELECT count() from event_0_time_func_batch group by toYYYYMM(time) 0.250s 0.159s 0.147s

SUMMARY: seems don't have to use date anymore (used to have to like us) https://github.com/yandex/ClickHouse/issues/2802
and please don't use single inserts
actually i've seen very weird behaviour from ch with many single inserts. you can kill
you server easy