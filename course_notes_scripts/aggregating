Preparation:
* rm data dir
* rm preprocessed configs
* slides
* start docker

Welcome back everyone.
Today we'll talk about extremely powerfull feature of ch

I mean I've sad a lot about how fast CH is, but the truth is,
not fast enough. I mean there's always ways to improve.
And CH team knows it. so they implemented aggregates.
Most of you know what aggregates are. I'll just quicly say that
aggregates are pretty much precompute values
Sometimes teams decide to precompute things with external job,
and save results to storage, like another db or a table
And it works. but this approach is not the best.
It's hard to maintain the jobs, and results could be hard to use on custom queries

CH allows us to


CREATE DATABASE aggregating

CREATE TABLE aggregating.event
(
  id UInt64,
  time DateTime,
  type UInt16,
  pokemon_id UInt16
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id);

lets start simple.
insert into event values(1, '2018-01-01 00:00:00', 1, 1);
insert into event values(1, '2018-01-01 00:00:00', 2, 1);
insert into event values(1, '2018-01-01 00:00:00', 2, 1);

now we can do
SELECT sumIf(type, type = 2)
FROM event
┌─sumIf(type, equals(type, 2))─┐
│                            4 │
└──────────────────────────────┘
or
SELECT countIf(type = 2)
FROM event
┌─countIf(equals(type, 2))─┐
│                        2 │
└──────────────────────────┘
simple enough
Lets explore state
SELECT countIfState(type = 2)
FROM event
┌─countIfState(equals(type, 2))─┐
│                               │
└───────────────────────────────┘
Make sense, and now
SELECT countIfMerge(x)
FROM
(
    SELECT countIfState(type = 2) AS x
    FROM event
)
┌─countIfMerge(x)─┐
│               2 │
└─────────────────┘

In production we want to create something like this
CREATE MATERIALIZED VIEW aggregating.event_aggs
ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(time)
ORDER BY (toYYYYMM(time), id)
AS SELECT
    id,
    time,
    sumState(type)    AS types_sum,
    uniqState(pokemon_id) AS unique_id
FROM aggregating.event
GROUP BY id, time;

quickly check
insert into event values(2, '2018-01-02 00:00:00', 1, 1);
insert into event values(2, '2018-01-02 00:00:00', 2, 1);
insert into event values(2, '2018-01-02 00:00:00', 2, 1);

select * from event where id=2
┌─id─┬────────────────time─┬─type─┬─pokemon_id─┐
│  2 │ 2018-01-02 00:00:00 │    1 │          1 │
│  2 │ 2018-01-02 00:00:00 │    2 │          1 │
│  2 │ 2018-01-02 00:00:00 │    2 │          1 │
└────┴─────────────────────┴──────┴────────────┘

And
SELECT
    sumMerge(types_sum),
    uniqMerge(unique_id)
FROM event_aggs
┌─sumMerge(types_sum)─┬─uniqMerge(unique_id)─┐
│                   5 │                    1 │
└─────────────────────┴──────────────────────┘

Ok, lets insert some data and do benchmarking with our selecter
config for inserter
TABLE_NAME = "event"
BULK_SIZE = 1000
EVENTS_PER_DAY = 1000
WORKERS = 5
DB_NAME = "aggregating"


def generate_random_event(event_date: datetime.datetime) -> dict:
    event_id = random.randint(0, 10)
    event_type = random.randint(0, 3)
    pokemon_id = random.randint(0, 10)
    event_datetime = event_date.replace(
        hour=random.randint(0, 23),
        minute=random.randint(0, 59))

    return {
        "id": event_id,
        "time": event_datetime,
        "type": event_type,
        "pokemon_id": pokemon_id
    }

benchmarking event.
SELECT sum(type) FROM aggregating.event
 attempt 0 took: 0.24256s
 attempt 1 took: 0.11743s
 attempt 2 took: 0.14443s

SELECT sumMerge(types_sum) FROM aggregating.event_aggs
 attempt 0 took: 0.21557s
 attempt 1 took: 0.1241s
 attempt 2 took: 0.16787s

todo: maybe use uniq. i think it's heavy
TODO: bigger dataset; use other partition on work laptop