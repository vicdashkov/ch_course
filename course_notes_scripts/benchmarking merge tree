Preparation:
* start ch
* code editor becnh scrips
* script to run bench with small events
* slides desktop
* delete data folder
* venv in created and activated for inserter
* browser with tabix

welcome back everyone.

In the previous lesson we got to know merge tree a little,
but we've actually gotten more questions then answers.
Like what do we do with all the order by and partition by clauses

It is a high time I intorduce you to 2 small scripts to kind of dirty
benchmark our tables. You can find a link to gitgub in the lecture notes

-- change to VS code with inserter script

This is not a python course, so I'll just breifly go over the code.

-- open inserter.py

In inserter.py we've got our, well, inserter.
This inserter simulates parallel insertion of specified number of workers
This way we sort of try to emulate production environment when you've got a pipeline with multiple,
perhaps, microservices.

-- open inserter configs

So yeah. This will just insert into our CH with configurable params like number of workers,
batch, size and so on.

-- open selecter.py

In selecter.py we've got our, well, selecter. Selecter will execute specified query specified number of times.

-- open selecter configs

Can you specify something else? not it selecter no.

To run our guys, I've created 2 simple bash scripts where we actully set all the configs.
For demonstartion purposes, values here are very small. For inserter we'll just do 10 events per day.

Later on we'll talk about larger numbers, when we'll try to compare different table configurations

Ok, to run the script let's create tables.

-- switch to tabix

I've got 6.

3 pairs of different configs. we use each pair to compare small batch vs large batch

For combination 1 we've got a time column, and we specify toYYYYMM expression for partition
Order by is tuple time, and id

I think this is good time to mention that ch will use
columns from order by as primary keys by default, you can specify custom primary key, and it has to be a prefix
of order by clouse.

For combination 2 we've got toYYYYMM expression for partition by and for order by

For combination 3 we'll create separate column to be used in partition by and order by columns
just as docs suggested a while back

Let's create all our tables

and run the tests and we'll come back to results once it's done

-- fast forward

Welcome back. Our insertion is complete.
I'f you followed along you noticed that it took us quite a bit of time
to insert paltry 3650 entries
And the blame goes to single inserts
CH really hates tham and performce poorly when we insert to often,
We'll talk about it a bit more a bit later

As of now lets run selecter.
and  I promise it won't take as long.

And it's done

let's take a look at the logs that our scripts generate

-- show insert log
inserter will tell us where it inserted; how much it inserted
and how long it took him to insert.

-- show select log
Selecter will execute specified query 3 times by default, and just display how
long it took to do so

These 2 logs should help us decide on the best configs for our tables

Additionaly We can get some idea about the sizes of our tables with this command

-- switch to tabix tab 2

Armed with new tools and trics,
we now can paint slightly better picture about our schemas.
In the backgroud I put 365000000 items into each of our tables
1000000 entries per day

I've assambled all the info and created this table

-- slides with tables

Just as a reminder;
tables with suffix single were bombarded with 1000 items per batch
and tables with suffix batch were bombardered with 100000 items per batch

So

Our batch size of 1000 perfoms poorly at insertion.
This is predictable. The timings are not as drammatic as I expected and could be attributed to network
But We can clearly see that big batches work good. So that's good.

Insertion into event_time_order_func took surpisingly little.
I was also amazed by the fact that inserting into event_date with separate date column was slower
than inserting into event_time with time expression.

When it comes to selects, event_date with group by on time column is surprasignly a big looser.
second comes event_time_order_func with expression in order by clause. This was very interesting to me.
I did not expected this result

I also run 2 different queries on event_data_batch tables.
One grouped by date and the other grouped by time
and group by date was almost twice as fast as group by time

And the most surprising numbers are the table sizes
event_time_order_func with time expression in order
by clouse takes the most space. I'm not quite sure why. Any ideas? let me know!

Many Good numbers here. Please play with it for yourself with your schemas and test your assumtions. I didn't
expect to see any surprises in this benchmark, and yet here we are.

-- swith to next slide

SUMMARY: it seems we don't have to use separate date column anymore
we can use toYYYYMM() or similar expression in partition by claouse,
as for order by and primary key, you really need to benchmark your schemas.

But, and this is a big one,
I would still use date column. Especilly in newer versions of CH which allows us to
be more flexible when specifying primary key other that order by key.
This could make a lot of different, and as we saw, operations on date column could be
seriously cheaper.

Also remember to never use single inserts.
I've seen very weird behaviour from ch with many single inserts even during today's small run. you can kill
you server easy. Rool of thumb that CH teams says somewhere - you probably don't want to insert more than once per second.

Hope you liked this video, and see you next on the next one